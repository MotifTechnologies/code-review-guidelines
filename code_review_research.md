# Code Review Bottleneck Solutions for High-Velocity Teams

## Research Summary
This document compiles modern practices for addressing code review bottlenecks in fast-moving engineering teams, based on practices from companies like Google, Facebook/Meta, Spotify, GitHub, and emerging industry patterns.

---

## 1. Async Code Review Practices & Tools

### Core Principles
- **Non-blocking by default**: Reviews shouldn't block forward progress
- **Time-boxed reviews**: Set expectations (e.g., first response within 4 hours)
- **Context-rich PRs**: Self-documenting changes reduce back-and-forth
- **Async-first communication**: Written explanations over synchronous meetings

### Best Practices

#### PR Description Template
```markdown
## What
[One-line summary]

## Why
[Business/technical context]

## How
[Implementation approach]

## Testing
[What was tested, how to verify]

## Risks
[What could break, monitoring plan]

## Screenshots/Demo
[For UI changes]
```

#### Review Efficiency Techniques
- **Pre-commit hooks**: Catch formatting/linting before review
- **CI gates**: Don't request review until tests pass
- **Self-review first**: Author reviews own PR, leaves comments on tricky parts
- **Small PRs**: Target <400 lines (studies show review quality drops after 400 LOC)
- **Reviewable commits**: Each commit tells a story, can be reviewed independently

### Tools & Automation

#### Modern Code Review Platforms
- **GitHub**: Advanced features (draft PRs, suggested changes, code owners)
- **Gerrit**: Change-based review (Google's choice)
- **Graphite**: Stacked PRs made easy
- **Reviewable**: Enhanced GitHub review experience
- **Phabricator**: Facebook's tool (now open source)

#### AI-Assisted Review
- **GitHub Copilot for PRs**: Auto-generates PR descriptions
- **CodeRabbit**: AI code reviewer (catches bugs, suggests improvements)
- **SonarQube**: Automated code quality checks
- **Codacy**: Automated code analysis
- **DeepCode/Snyk Code**: Security-focused AI review

#### Review Automation Tools
- **Danger**: Automates common review comments
- **Pronto**: Runs multiple code checkers
- **ReviewDog**: Posts review comments from any linter
- **Conventional Comments**: Standardizes review comment types

---

## 2. Tiered Review Systems

### Three-Tier Model

#### Tier 1: Auto-Merge (No Human Review)
**Criteria:**
- Docs-only changes (typos, clarifications)
- Test-only additions
- Dependency version bumps (non-major, passing tests)
- Configuration tweaks (with test coverage)
- Automated refactors (IDE-generated)

**Requirements:**
- All CI checks pass
- Branch up-to-date with main
- No conflicts
- Automated security scans pass

**Tools:**
- **Mergify**: Rule-based auto-merge
- **Kodiak**: GitHub auto-merge bot
- **Bors**: Testing & merging bot
- **GitHub Auto-merge**: Built-in feature

**Example Mergify Config:**
```yaml
pull_request_rules:
  - name: Auto-merge docs changes
    conditions:
      - author=@team-members
      - files~=^docs/
      - check-success=CI
      - #approved-reviews-by>=1
    actions:
      merge:
        method: squash
```

#### Tier 2: Quick Review (1 approval, <30 min SLA)
**Criteria:**
- Small bug fixes (<50 lines)
- Minor feature additions
- Refactoring within one module
- Low-risk changes to non-critical paths
- Changes by senior engineers

**Review Focus:**
- Does it solve the stated problem?
- Are there obvious bugs?
- Basic style/readability check
- Trust but verify

**Process:**
- Use "LGTM with nits" - don't block on minor issues
- Approve + leave comments for follow-up
- Author fixes in follow-up PR

#### Tier 3: Full Review (2+ approvals, deeper scrutiny)
**Criteria:**
- API/interface changes
- Security-sensitive code
- Database migrations
- Performance-critical paths
- Architecture changes
- Changes by junior engineers (mentoring)

**Review Requirements:**
- Domain expert approval
- Security review (if applicable)
- Performance testing results
- Documentation updated
- Backwards compatibility verified

### Risk-Based Classification

**Automated Risk Scoring:**
```python
risk_score = (
    lines_changed * 0.1 +
    num_files * 5 +
    (50 if touches_auth else 0) +
    (50 if touches_db_schema else 0) +
    (30 if touches_api else 0) +
    (-20 if has_tests else 0) +
    (-10 if senior_author else 0)
)

if risk_score < 20:
    tier = "auto-merge"
elif risk_score < 50:
    tier = "quick-review"
else:
    tier = "full-review"
```

---

## 3. Stacked PRs & Ship-Show-Ask Model

### Ship/Show/Ask Model

A mental model for deciding review approach, popularized by Rouan Wilsenach at Martinfowler.com.

#### Ship (0% Review)
**When to use:**
- Trivial changes
- High-confidence changes by experienced engineers
- Time-sensitive fixes
- Experimental work that can be reverted

**Process:**
1. Make change
2. Merge to main
3. Notify team (post-merge notification)
4. Monitor for issues

**Safety nets:**
- Feature flags (can disable quickly)
- Comprehensive test suite
- Good monitoring/alerting
- Easy rollback

#### Show (Async Review, Non-Blocking)
**When to use:**
- Medium-risk changes
- New patterns you want team to learn
- Refactorings
- Default for most work

**Process:**
1. Create PR
2. Merge immediately (or within hours)
3. Request review asynchronously
4. Address feedback in follow-up PRs

**Benefits:**
- No blocking on reviewers
- Faster iteration
- Review becomes learning, not gating

#### Ask (Traditional Blocking Review)
**When to use:**
- High-risk changes
- API contracts
- Security-sensitive code
- Areas where you want mentorship

**Process:**
1. Create PR
2. Request review
3. Wait for approval(s)
4. Address feedback
5. Merge after approval

### Stacked PRs (Dependent Changes)

**Problem:** Large features require multiple PRs, but traditional flow blocks each PR on review.

**Solution:** Stack PRs on top of each other, review in parallel.

#### Traditional Flow (Slow)
```
PR1 â†’ Wait for review â†’ Merge â†’ PR2 â†’ Wait for review â†’ Merge â†’ PR3
Timeline: 3-5 days
```

#### Stacked Flow (Fast)
```
PR1 â† PR2 â† PR3 (all created at once)
â†“      â†“      â†“
Review concurrently
Timeline: 1 day
```

#### Tools for Stacked PRs
- **Graphite**: Purpose-built for stacked PRs
- **Gerrit**: Native support for change chains
- **git-branchless**: CLI tool for stacked changes
- **ghstack**: GitHub stacked PRs tool
- **Aviator**: Stacked PRs + smart merging

#### Best Practices
1. **Each PR should be independently valuable**: Can merge just PR1, or PR1+PR2
2. **Keep them small**: Stack of 5x100-line PRs > 1x500-line PR
3. **Clear dependencies**: Document which PRs depend on which
4. **Update downstream PRs**: When PR1 changes, rebase PR2, PR3
5. **Review bottom-up**: Review PR1 first, then PR2, etc.

#### Example Workflow with Graphite
```bash
# Create stack
gt branch create "add-user-model"
# Make changes, commit
gt branch create "add-user-api"
# Make changes, commit
gt branch create "add-user-ui"
# Make changes, commit

# Submit entire stack
gt stack submit

# Review happens in parallel
# Merge happens bottom-up automatically
```

---

## 4. Trunk-Based Development with Feature Flags

### Core Principles
- Merge to main/trunk multiple times per day
- Keep feature branches short-lived (<1 day)
- Hide incomplete features behind flags
- Review becomes continuous, not batch

### Feature Flag Strategy

#### Types of Flags
1. **Release flags**: Control feature rollout
2. **Experiment flags**: A/B testing
3. **Ops flags**: Kill switches for performance
4. **Permission flags**: Gradual access rollout
5. **Development flags**: Hide incomplete work

#### Feature Flag Lifecycle
```
Development â†’ Team Testing â†’ Beta Users â†’ % Rollout â†’ 100% â†’ Remove Flag
```

#### Tools
- **LaunchDarkly**: Enterprise feature management
- **Split.io**: Feature delivery platform
- **Unleash**: Open-source feature toggle system
- **Flagsmith**: Open-source with dashboard
- **Custom**: Simple in-house system (env vars + config)

#### Best Practices
```python
# Good: Default to safe state
if feature_flags.is_enabled('new_checkout_flow', user_id=user.id):
    return new_checkout()
else:
    return old_checkout()

# Bad: Flag in the middle of logic
def checkout():
    # ... 50 lines ...
    if feature_flags.is_enabled('new_step'):
        new_logic()
    # ... 50 more lines ...
```

### Review Process with Trunk-Based Development

#### Small, Frequent Merges
- **Target**: Merge 2-5x per day per engineer
- **PR size**: <200 lines
- **Review time**: <1 hour turnaround

#### Pre-Merge Testing
```yaml
# CI pipeline
- Lint & Format Check
- Unit Tests
- Integration Tests
- Build & Package
- Deploy to Preview Environment
- E2E Tests
- Security Scan
- Merge
- Deploy to Staging (with flag off)
- Monitor for issues
- Gradually enable flag
```

### Benefits for Review
- Smaller changes = faster reviews
- Feature flags = can merge incomplete work safely
- Continuous integration = fewer merge conflicts
- Team sees changes immediately

---

## 5. Experimental Branches & Fast-Track Development

### The Challenge
Some work needs to move faster than review can keep up (prototypes, spikes, research).

### Strategy 1: Experimental Branch with Periodic Syncs

#### Setup
```
main (protected, reviewed)
  â†“
experimental/new-architecture (fast-moving, minimal review)
  â†“ (periodic sync)
main
```

#### Process
1. **Fast development**: Team works on experimental branch
2. **Daily merges**: Pull from main to stay current
3. **Weekly reviews**: Review diffs in batches
4. **Graduation**: When stable, rebase/squash and merge to main

#### Review Strategy
- **During experiment**: Pair programming, mob sessions (not async review)
- **At graduation**: Full review of final state + migration plan

### Strategy 2: Parallel Track with Feature Flag

#### Setup
```python
if feature_flags.is_enabled('experimental_track'):
    return experimental_implementation()
else:
    return stable_implementation()
```

#### Benefits
- Both implementations in main branch
- Can A/B test in production
- No branch management
- Gradual migration

### Strategy 3: Fork & Periodic Reimplementation

For radical experiments:
1. **Fork**: Create separate repo/branch
2. **Experiment**: Move fast, break things
3. **Learnings**: Document what worked
4. **Reimplement**: Build production version in main repo with proper review
5. **Discard** experimental code

### Review SLA for Experimental Work
- **No blocking review**: Can merge without approval
- **Post-merge review**: Team reviews 1x per week
- **Pair programming**: Encouraged for knowledge sharing
- **Documentation requirement**: Every merge needs explanation
- **Rollback plan**: Must be trivial to disable

---

## 6. Review SLAs & Metrics That Work

### SLA Tiers

#### Tier-Based SLA
| Tier | First Response | Approval | Merge |
|------|---------------|----------|-------|
| Auto-merge | Immediate | N/A | Immediate |
| Quick Review | 2 hours | 4 hours | 6 hours |
| Standard Review | 4 hours | 1 day | 1.5 days |
| Full Review | 8 hours | 2 days | 3 days |

#### Urgency-Based SLA
| Urgency | Response | Example |
|---------|----------|---------|
| Critical (P0) | 30 min | Production outage fix |
| High (P1) | 2 hours | Customer-blocking bug |
| Normal (P2) | 4 hours | Feature work |
| Low (P3) | 1 day | Tech debt, refactoring |

### Metrics to Track

#### Health Metrics (Good)
- **Time to First Review**: How fast do reviewers respond?
  - Target: <4 hours during business hours
- **PR Cycle Time**: Time from creation to merge
  - Target: <24 hours for 80% of PRs
- **PR Size Distribution**: Are PRs small?
  - Target: 80% under 400 lines
- **Review Load Balance**: Are reviews distributed evenly?
  - Target: No reviewer >30% of all reviews
- **Merge Frequency**: How often does team merge?
  - Target: >2 merges/engineer/day

#### Anti-Metrics (Be Careful)
- **Review Speed**: Can encourage rubber-stamping
- **Number of Comments**: Can discourage thorough review
- **Approval Rate**: Doesn't measure quality

### Dashboard Example

```markdown
## Weekly Review Health (Team: Payments)

### Cycle Time
- P50: 4.2 hours âœ… (target: <6h)
- P90: 18.5 hours âš ï¸ (target: <24h)
- P99: 52.3 hours âŒ (target: <48h)

### PR Size
- <100 lines: 45% âœ…
- 100-400 lines: 40% âœ…
- 400+ lines: 15% âš ï¸

### Review Load
- Alice: 23 reviews (balanced)
- Bob: 8 reviews (low - capacity issue?)
- Carol: 31 reviews (high - bottleneck?)

### Blockers
- 3 PRs waiting >48 hours
- 2 PRs missing required security review
```

### Implementing SLAs

#### Automated Notifications
```yaml
# GitHub Actions example
- name: Check Review SLA
  if: hours_since_created > 4 && review_count == 0
  run: |
    # Notify in Slack
    # Escalate to team lead
    # Add "needs-review" label
```

#### Review Rotation
```python
# Assign reviewers based on:
# 1. Current load (fewer pending reviews = higher priority)
# 2. Expertise (code owners)
# 3. Time zone (available now)
# 4. Learning goals (junior wants to learn this area)

def assign_reviewer(pr):
    reviewers = get_eligible_reviewers(pr.files)
    reviewers.sort(key=lambda r: (
        r.pending_review_count,  # Load balancing
        -r.expertise_score(pr),  # Prefer experts
        -r.availability_score(),  # Prefer online
    ))
    return reviewers[0]
```

#### Team Norms
- **Review Time**: Block 9-10am daily for reviews
- **Rotation**: On-call reviewer each day (responds to all PRs)
- **Pairing**: Complex PRs get synchronous review (pair/mob)
- **Escalation**: PRs blocked >24h auto-escalate to team lead

---

## 7. Tools & Automation to Reduce Review Burden

### Pre-Review Automation

#### Auto-Formatting
```yaml
# .github/workflows/format.yml
name: Auto-format
on: pull_request
jobs:
  format:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - run: npm run format
      - uses: stefanzweifel/git-auto-commit-action@v4
        with:
          commit_message: "Auto-format code"
```

**Tools:**
- **Prettier**: JavaScript/TypeScript
- **Black**: Python
- **rustfmt**: Rust
- **gofmt**: Go
- **clang-format**: C/C++

#### Automated Code Analysis
```yaml
# Pre-commit hooks
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
  - repo: https://github.com/psf/black
    hooks:
      - id: black
  - repo: https://github.com/PyCQA/flake8
    hooks:
      - id: flake8
```

### Review-Time Automation

#### Conventional Comments
Standardize review comments for clarity:

```
suggestion: Use a more descriptive variable name
This would improve readability.

question: Why did you choose this approach?
I'm curious about the tradeoff here.

nitpick (non-blocking): Extra whitespace
Not important, but noticed it.

issue (blocking): This will cause a memory leak
The listener is never removed.

praise: Nice refactoring!
This is much clearer than before.
```

**Tool:** Browser extension or review template

#### Danger (Automated Reviewer)
```javascript
// dangerfile.js
import { danger, warn, fail, message } from 'danger'

// No PR is too small for a description
if (!danger.github.pr.body || danger.github.pr.body.length < 10) {
  fail('Please add a description to your PR.')
}

// Big PRs are hard to review
const bigPRThreshold = 400
if (danger.github.pr.additions + danger.github.pr.deletions > bigPRThreshold) {
  warn(':exclamation: Big PR. Consider breaking this into smaller PRs.')
}

// Always update tests
const hasAppChanges = danger.git.modified_files.some(f => f.match(/^src\//))
const hasTestChanges = danger.git.modified_files.some(f => f.match(/^tests\//))
if (hasAppChanges && !hasTestChanges) {
  warn('This PR modifies app code but no tests were updated.')
}

// Remind about docs
if (hasAppChanges && !danger.git.modified_files.includes('CHANGELOG.md')) {
  message('Consider updating the CHANGELOG.')
}
```

#### AI Code Review Tools

**GitHub Copilot for PRs**
- Auto-generates PR descriptions
- Summarizes changes
- Suggests test cases

**CodeRabbit**
- Line-by-line AI review
- Catches common bugs
- Suggests improvements
- Learns team's patterns

**Sourcery (Python)**
- Refactoring suggestions
- Code quality improvements
- Integrated with GitHub

**SonarCloud**
- Security vulnerabilities
- Code smells
- Technical debt tracking
- Quality gates

### Post-Review Automation

#### Auto-Merge After Approval
```yaml
# Kodiak config (.kodiak.toml)
[merge]
automerge_label = "automerge"
method = "squash"
delete_branch_on_merge = true
optimistic_updates = true

[merge.message]
title = "pull_request_title"
body = "pull_request_body"

[approve]
auto_approve_usernames = ["dependabot"]
```

#### Automated Dependency Updates
- **Dependabot**: Auto-creates PRs for dependency updates
- **Renovate**: More configurable than Dependabot
- **Snyk**: Security-focused updates

**Auto-merge safe updates:**
```json
{
  "packageRules": [
    {
      "matchUpdateTypes": ["patch"],
      "automerge": true
    }
  ]
}
```

### Review Workflow Tools

#### GitHub Features
- **Draft PRs**: Signal "not ready for review"
- **Code Owners**: Auto-assign reviewers
- **Required Reviews**: Enforce policy
- **Suggested Changes**: One-click apply
- **Review Assignment**: Round-robin or load-balanced

#### Slack Integration
```
New PR from @alice: "Add user authentication" ðŸ”
Size: 245 lines | Tier: Standard Review
Reviewers needed: @security-team
Review by: 2pm today
[View PR] [Claim Review]
```

#### Review Analytics Dashboard

**Key Views:**
1. **My Queue**: PRs waiting for my review
2. **Team Queue**: PRs needing any review
3. **Blocked PRs**: Waiting >SLA
4. **My PRs**: Status of my submitted PRs
5. **Trends**: Team velocity over time

**Tools:**
- **LinearB**: Engineering metrics platform
- **Swarmia**: Developer productivity
- **Haystack**: Engineering metrics
- **Jellyfish**: Engineering management platform
- **Custom**: Build with GitHub API + Dashboard tool

---

## 8. Implementation Roadmap

### Phase 1: Quick Wins (Week 1-2)
1. **Set up auto-formatting**: No more style debates
2. **Install Danger**: Automated PR checks
3. **Define PR tiers**: Auto-merge criteria
4. **Set SLA expectations**: Team agreement on response times
5. **Review time block**: Daily 30-min review time

**Expected Impact:** 20-30% reduction in review time

### Phase 2: Process Changes (Week 3-6)
1. **Implement Ship/Show/Ask**: Train team on model
2. **Enable auto-merge**: Start with docs/tests
3. **PR size limits**: Enforce <400 lines (with exceptions)
4. **Stacked PRs pilot**: Try with one team
5. **Review rotation**: Dedicated reviewer each day

**Expected Impact:** 40-50% reduction in review cycle time

### Phase 3: Advanced Tooling (Week 7-12)
1. **AI code review**: CodeRabbit or similar
2. **Feature flags**: Infrastructure setup
3. **Trunk-based development**: Move from long-lived branches
4. **Metrics dashboard**: Track review health
5. **Continuous deployment**: Reduce merge-to-production time

**Expected Impact:** 60-70% reduction in cycle time, 2-3x more deploys/day

### Phase 4: Culture & Optimization (Ongoing)
1. **Blameless postmortems**: Learn from mistakes
2. **Review training**: Teach effective review techniques
3. **Pair programming**: For complex changes
4. **Team retrospectives**: Continuous improvement
5. **Celebrate speed**: Recognize fast, quality reviews

**Expected Impact:** Sustained high velocity, improved team satisfaction

---

## 9. Case Studies

### Google's Code Review Process
- **Tool**: Gerrit (change-based review)
- **Rule**: All code reviewed before merge
- **Speed**: Average 4 hours from upload to submit
- **Techniques**:
  - Owners files (auto-assign reviewers)
  - Small changes encouraged
  - Tools auto-fix formatting
  - Bots provide instant feedback
  - Reviewers get metrics on response time

**Key Insight:** High standards AND high speed are compatible with good tooling.

### Meta/Facebook's Approach
- **Tool**: Phabricator (now Sapling/GitHub)
- **Speed**: Median 2 hours to first response
- **Techniques**:
  - Land/commit directly to main (with review)
  - Extensive automated testing
  - Easy rollback
  - Stacked diffs native to workflow
  - Review as learning, not gatekeeping

**Key Insight:** Trust engineers + great testing = fast iteration.

### Spotify's Model
- **Philosophy**: Squad autonomy
- **Review**: Team decides own process
- **Common pattern**: Ship/Show/Ask with heavy Show usage
- **Techniques**:
  - Feature flags everywhere
  - Trunk-based development
  - Continuous deployment
  - Post-deploy review common

**Key Insight:** Let teams optimize for their context.

### Vercel's Approach
- **Speed**: Average merge time <1 hour
- **Techniques**:
  - Preview deployments for every PR
  - Visual regression testing
  - Auto-merge for safe changes
  - Small PRs (<100 lines average)
  - Review is asynchronous, non-blocking

**Key Insight:** Make review environment identical to production.

---

## 10. Common Pitfalls & How to Avoid

### Pitfall 1: "Fast = Low Quality"
**Myth:** Slower review = more thorough review
**Reality:** Long review times = context loss, reduced focus
**Solution:** Time-box reviews (30 min max), focus on high-impact issues

### Pitfall 2: Rubber-Stamping
**Risk:** Speed pressure leads to approving without reading
**Solution:**
- Randomized audit of reviews
- Require comments (not just approval)
- Track bug escape rate
- Pair programming for risky changes

### Pitfall 3: Review Theater
**Problem:** Detailed comments on trivial issues, missing real bugs
**Solution:**
- Educate on what matters (logic > style)
- Auto-fix style issues
- Focus review on: correctness, security, performance, maintainability

### Pitfall 4: Bottleneck Reviewers
**Problem:** Only 1-2 people can review certain areas
**Solution:**
- Knowledge sharing sessions
- Pair programming
- Documentation
- Gradual ownership expansion
- Accept slightly less expert reviews

### Pitfall 5: Review Debt
**Problem:** PRs pile up, team gives up on timely review
**Solution:**
- Stop feature work, clear backlog
- Review day (whole team focuses on reviews)
- Tighten SLAs going forward
- Close stale PRs

---

## 11. Recommended Reading & Resources

### Articles
- **Ship / Show / Ask** - Rouan Wilsenach (Martin Fowler's blog)
- **How to Review Code** - Google Engineering Practices
- **The Art of Code Review** - Palantir Blog
- **Trunk Based Development** - trunkbaseddevelopment.com

### Books
- **Software Engineering at Google** - Chapters on code review
- **Accelerate** - Research on high-performing teams
- **The DevOps Handbook** - Continuous delivery practices

### Tools to Explore
- **Graphite**: Stacked PRs - graphite.dev
- **Gerrit**: Google's review tool - gerritcodereview.com
- **CodeRabbit**: AI code review - coderabbit.ai
- **LaunchDarkly**: Feature flags - launchdarkly.com
- **Danger**: PR automation - danger.systems
- **Mergify**: Auto-merge - mergify.com

---

## 12. Summary: Key Takeaways

### The 80/20 of Fast Code Review

#### Do These First (80% Impact)
1. **Set up auto-merge for safe changes** (docs, tests, deps)
2. **Auto-format code** (eliminate style debates)
3. **Keep PRs small** (<400 lines)
4. **Set SLA expectations** (4-hour first response)
5. **Use feature flags** (merge incomplete work safely)

#### Cultural Shifts Required
- **Trust over control**: Assume good intent
- **Speed is quality**: Faster feedback = better code
- **Continuous improvement**: Merge small, iterate fast
- **Review is learning**: Not just gatekeeping
- **Automate everything**: Humans for what matters

#### Metrics to Track
- **Cycle time**: PR creation to merge (<24h target)
- **PR size**: 80% under 400 lines
- **Review SLA**: 90% get first response <4h
- **Deploy frequency**: Multiple per day

#### When to Use What

| Situation | Approach |
|-----------|----------|
| Docs change | Auto-merge |
| Small bug fix | Quick review (Ship/Show) |
| New feature | Show (post-merge review) |
| API change | Ask (blocking review) |
| Security change | Ask (multiple reviewers) |
| Prototype | Experimental branch or feature flag |
| Large feature | Stack PRs (5x100 lines) |

---

## Conclusion

Code review doesn't have to be a bottleneck. By combining:
- **Tiered review** (not everything needs deep review)
- **Automation** (catch trivial issues before human review)
- **Feature flags** (merge safely, deploy gradually)
- **Stacked PRs** (review in parallel)
- **Clear SLAs** (set expectations)
- **Trust + testing** (move fast without breaking things)

...teams can achieve both high velocity AND high quality.

The key is **reducing time-to-merge while maintaining or improving quality through automation, testing, and smart risk assessment**.

Start with quick wins (auto-format, auto-merge docs), then gradually shift culture toward trunk-based development with continuous deployment.
